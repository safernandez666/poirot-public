services:

  # ============================================================================
  # CORE — always started (docker compose up -d)
  # ============================================================================

  hawk-scanner:
    build: .
    container_name: hawk-scanner
    platform: linux/amd64
    extra_hosts:
      - "host.docker.internal:host-gateway"
    env_file: .env
    environment:
      - PYTHONIOENCODING=utf-8
      - LANG=C.UTF-8
      - LC_ALL=C.UTF-8
    volumes:
      - ./hawk-scanner/connection.yml:/app/connection.yml
      - ./hawk-scanner/fingerprint.yml:/app/fingerprint.yml
      - ./hawk-scanner/notification_manager.py:/app/notification_manager.py
      - ./hawk-scanner/data:/app/data
    command: tail -f /dev/null
    networks:
      - hawk-network

  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: hawk-dashboard
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      hawk-scanner:
        condition: service_started
    ports:
      - "8080:80"
    environment:
      - ALERTS_DB_PATH=/app/data/alerts.db
      - FINGERPRINT_PATH=/app/config/fingerprint.yml
      - CONNECTION_PATH=/app/config/connection.yml
      - FLASK_DEBUG=false
      - THEHIVE_ENABLED=${THEHIVE_ENABLED:-false}
      - THEHIVE_URL=http://thehive:9000
      - THEHIVE_API_KEY=${THEHIVE_API_KEY:-}
    volumes:
      - ./hawk-scanner/data:/app/data
      - ./hawk-scanner/fingerprint.yml:/app/config/fingerprint.yml
      - ./hawk-scanner/connection.yml:/app/config/connection.yml
      - ./hawk-scanner/notification_manager.py:/app/notification_manager.py
      - ./.env:/app/.env
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - hawk-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ============================================================================
  # KAFKA — always started (Redpanda, Kafka-compatible, no ZooKeeper)
  # ============================================================================

  redpanda:
    image: redpandadata/redpanda:v23.3.21
    container_name: redpanda
    command:
      - redpanda
      - start
      - --smp
      - "1"
      - --memory
      - 512M
      - --mode
      - dev-container
      - --default-log-level=warn
      - --kafka-addr
      - internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr
      - internal://redpanda:9092,external://localhost:19092
    ports:
      - "19092:19092"
    networks:
      - hawk-network
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster info --brokers localhost:9092 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  kafka-setup:
    build: .
    container_name: kafka-setup
    platform: linux/amd64
    depends_on:
      redpanda:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=redpanda:9092
    volumes:
      - ./hawk-scanner/kafka_demo_producer.py:/app/kafka_demo_producer.py
    command: python /app/kafka_demo_producer.py
    restart: "no"
    networks:
      - hawk-network

  # ============================================================================
  # DEMO — started with: docker compose --profile demo up -d
  # Includes sample data sources (MySQL, S3) + automatic data seeding
  # ============================================================================

  hawk-mysql:
    image: mysql:8.0
    container_name: hawk-mysql
    profiles: ["demo"]
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: pocdb
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-prootpassword"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - hawk-network

  localstack:
    image: localstack/localstack:2.2
    container_name: localstack
    profiles: ["demo"]
    environment:
      - SERVICES=s3
      - DEFAULT_REGION=us-east-1
      - DEBUG=1
    ports:
      - "4566:4566"
    volumes:
      - "./localstack_data:/tmp/localstack"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - hawk-network

  demo-setup:
    build: .
    container_name: demo-setup
    profiles: ["demo"]
    platform: linux/amd64
    depends_on:
      hawk-mysql:
        condition: service_healthy
      localstack:
        condition: service_healthy
    environment:
      - MYSQL_HOST=hawk-mysql
      - MYSQL_PORT=3306
      - MYSQL_USER=root
      - MYSQL_PASSWORD=rootpassword
      - MYSQL_DATABASE=pocdb
      - S3_ENDPOINT=http://localstack:4566
      - S3_BUCKET=poc-bucket
    volumes:
      - ./scripts/demo_setup.py:/app/demo_setup.py
    command: python /app/demo_setup.py
    restart: "no"
    networks:
      - hawk-network

  # ============================================================================
  # OPTIONAL — TheHive (docker compose --profile thehive up -d)
  # ============================================================================

  cassandra:
    image: cassandra:4.1
    container_name: cassandra
    profiles: ["thehive"]
    environment:
      - MAX_HEAP_SIZE=1G
      - HEAP_NEWSIZE=256M
      - CASSANDRA_CLUSTER_NAME=thehive
    volumes:
      - cassandra_data:/var/lib/cassandra
    networks:
      - hawk-network
    healthcheck:
      test: ["CMD-SHELL", "cqlsh -e 'describe cluster' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.9
    container_name: elasticsearch
    profiles: ["thehive"]
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - cluster.name=thehive
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - hawk-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  thehive:
    image: strangebee/thehive:5.0
    platform: linux/amd64
    container_name: thehive
    profiles: ["thehive"]
    depends_on:
      cassandra:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    ports:
      - "9000:9000"
    environment:
      - JVM_OPTS=-Xms1G -Xmx1G -Dhttp.address=0.0.0.0 -Djava.net.preferIPv4Stack=true
    volumes:
      - thehive_data:/opt/thehive/data
      - ./thehive-config/application.conf:/etc/thehive/application.conf:ro
    networks:
      - hawk-network
    restart: unless-stopped
    healthcheck:
      # TheHive /api/v1/status returns 401 without auth — that still means the server is up
      test: ["CMD-SHELL", "curl -s -o /dev/null -w '%{http_code}' http://localhost:9000/api/v1/status | grep -qE '200|401'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

  # ============================================================================
  # OPTIONAL — Ollama AI (docker compose --profile ollama up -d)
  # ============================================================================

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles: ["ollama"]
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
    networks:
      - hawk-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    entrypoint: >
      sh -c "
        /bin/ollama serve &
        sleep 5
        echo 'Downloading llama3.2:3b...'
        ollama pull llama3.2:3b
        echo 'Model ready'
        wait
      "

networks:
  hawk-network:
    driver: bridge

volumes:
  mysql_data:
  cassandra_data:
  elasticsearch_data:
  thehive_data:
  hawk-data:
  ollama_data:
